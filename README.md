# Expo 2025 Pavilion Wait Time Scraper

ãƒ‘ãƒ“ãƒªã‚ªãƒ³ã®å¾…ã¡æ™‚é–“ã‚’1æ™‚é–“ã”ã¨ã«å–å¾—ã—ã€CSVã«ä¿å­˜ã—ã¾ã™ã€‚

## å®Ÿè¡Œæ–¹æ³•

```bash
uv venv
uv pip sync
python scrape_wait_times.py
```

## ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- requests
- beautifulsoup4

ä»¥ä¸‹ã«ã€ã“ã‚Œã¾ã§ã®å†…å®¹ã‚’ **ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘ã®è©³ç´°ãªæŠ€è¡“æŒ‡ç¤ºæ›¸** ã¨ã—ã¦ã¾ã¨ã‚ã¾ã—ãŸã€‚
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ¦‚è¦ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆã€é–‹ç™ºç’°å¢ƒã€GitHub Actions ã«ã‚ˆã‚‹å®šæœŸå®Ÿè¡Œã¾ã§ã€ã™ã¹ã¦ç¶²ç¾…ã—ã¦ã„ã¾ã™ã€‚

---

# ğŸ“„ Expo 2025 ãƒ‘ãƒ“ãƒªã‚ªãƒ³å¾…ã¡æ™‚é–“ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ä»•æ§˜æ›¸

## âœ… æ¦‚è¦

Expo 2025 å¤§é˜ªãƒ»é–¢è¥¿ä¸‡åšã®ã€Œãƒ‘ãƒ“ãƒªã‚ªãƒ³å¾…ã¡æ™‚é–“ã€æƒ…å ±ã‚’ã€Webä¸Šã‹ã‚‰1æ™‚é–“ã”ã¨ã«è‡ªå‹•å–å¾—ã—ã€CSVãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜ã—ã¦ä¿å­˜ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚GitHub Actions ã‚’åˆ©ç”¨ã—ã¦å®šæœŸå®Ÿè¡Œã•ã‚Œã€å–å¾—ã—ãŸCSVã¯GitHubä¸Šã§ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã•ã‚Œã¾ã™ã€‚

---

## ğŸ—ï¸ æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ãƒ»æ§‹æˆæ–¹é‡

| é …ç›®      | å†…å®¹                                                                      |
| ------- | ----------------------------------------------------------------------- |
| è¨€èª      | Python 3.11ä»¥ä¸Š                                                           |
| ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç®¡ç† | [`uv`](https://astral.sh/blog/uv-python-package-manager/)ï¼ˆPoetryé¢¨ã®é«˜é€Ÿä»£æ›¿ï¼‰ |
| ä»®æƒ³ç’°å¢ƒ    | `uv venv` ã«ã‚ˆã‚‹ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒæ§‹ç¯‰                                                   |
| ã‚½ãƒ¼ã‚¹ç®¡ç†   | Git + GitHub                                                            |
| å®šæœŸå®Ÿè¡Œ    | GitHub Actionsï¼ˆç„¡æ–™æ  2000åˆ†/æœˆï¼‰                                             |
| ãƒ‡ãƒ¼ã‚¿ä¿å­˜   | CSVãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒªãƒã‚¸ãƒˆãƒªç›´ä¸‹ã§Gitã«ã‚³ãƒŸãƒƒãƒˆï¼‰                                               |

---

## ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ

```
expo-wait-scraper/
â”œâ”€â”€ scrape_wait_times.py        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æœ¬ä½“
â”œâ”€â”€ wait_times.csv              # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæ¯å›è¿½è¨˜ï¼‰
â”œâ”€â”€ pyproject.toml              # Pythonä¾å­˜å®šç¾©ï¼ˆuvç”¨ï¼‰
â”œâ”€â”€ uv.lock                     # lockãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆè‡ªå‹•ç”Ÿæˆï¼‰
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ scrape.yml          # GitHub Actions è¨­å®š
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ§‘â€ğŸ’» ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ`scrape_wait_times.py`ï¼‰

```python
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import csv

URL = "https://expo2025.fun/%E3%83%91%E3%83%93%E3%83%AA%E3%82%AA%E3%83%B3%E5%BE%85%E3%81%A1%E6%99%82%E9%96%93/"
CSV_FILE = "wait_times.csv"

def scrape_wait_times():
    response = requests.get(URL)
    response.encoding = "utf-8"
    soup = BeautifulSoup(response.text, "html.parser")

    rows = []
    for row in soup.select("div.table-responsive tbody tr"):
        cols = row.find_all("td")
        if len(cols) >= 2:
            name = cols[0].text.strip()
            wait_time = cols[1].text.strip()
            now = datetime.now().isoformat()
            rows.append([now, name, wait_time])

    with open(CSV_FILE, "a", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerows(rows)

if __name__ == "__main__":
    scrape_wait_times()
```

---

## âš™ï¸ `pyproject.toml`

```toml
[project]
name = "expo-wait-scraper"
version = "0.1.0"
description = "Scrapes Expo 2025 pavilion wait times"
requires-python = ">=3.10"

dependencies = [
    "requests",
    "beautifulsoup4"
]
```

---

## ğŸ“Œ `.gitignore`

```gitignore
__pycache__/
*.pyc
.venv/
```

â€» `wait_times.csv` ã¯GitHubã§ç®¡ç†ã™ã‚‹ãŸã‚é™¤å¤–ã—ãªã„

---

## â° GitHub Actions ã®è¨­å®šï¼ˆ`.github/workflows/scrape.yml`ï¼‰

```yaml
name: Scrape Wait Times

on:
  schedule:
    - cron: '0 * * * *'  # æ¯æ™‚00åˆ†ï¼ˆUTCï¼‰
  workflow_dispatch:     # æ‰‹å‹•å®Ÿè¡Œã‚‚å¯èƒ½

jobs:
  run-scraper:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install requests beautifulsoup4

      - name: Run scraper
        run: python scrape_wait_times.py

      - name: Commit and push CSV
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add wait_times.csv
          git commit -m "Update wait_times.csv ($(date -u))" || echo "No changes to commit"
          git push
```

---

## ğŸš€ åˆå›ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †ï¼ˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘ï¼‰

### 1. uv ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆã¾ã ãªã‚‰ï¼‰

```bash
curl -Ls https://astral.sh/uv/install.sh | sh
```

### 2. ä»®æƒ³ç’°å¢ƒä½œæˆã¨ä¾å­˜ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
uv venv
source .venv/bin/activate
uv pip install requests beautifulsoup4
```

### 3. GitHub ãƒªãƒã‚¸ãƒˆãƒªä½œæˆã¨ push

```bash
git init
gh auth login                      # GitHub CLIãŒå¿…è¦
gh repo create expo-wait-scraper --public --source=. --push
```

---

## ğŸ“¦ GitHub Actions å®Ÿè¡Œå¾Œã®æˆæœç‰©

* `wait_times.csv` ãŒ1æ™‚é–“ã”ã¨ã«è¿½è¨˜ã•ã‚Œã€GitHubã«è‡ªå‹•ã‚³ãƒŸãƒƒãƒˆãƒ»ãƒ—ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã™
* æ‰‹å‹•ã§ã®å®Ÿè¡Œãƒ†ã‚¹ãƒˆã‚‚ã€ŒActionsã€ã‚¿ãƒ– â†’ `Run workflow` ã‹ã‚‰å¯èƒ½ã§ã™

---

## ğŸ§© æ‹¡å¼µã®ä½™åœ°ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰

* æœˆå˜ä½ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†å‰²ï¼š`wait_times_YYYY-MM.csv`
* Google Drive ã‚„ S3 ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
* GitHub Pages ã‚„ Superset ãªã©ã§ã®å¯è¦–åŒ–
* HTMLæ§‹é€ å¤‰æ›´ã¸ã®è€æ€§å¼·åŒ–ï¼ˆãƒ‘ãƒ“ãƒªã‚ªãƒ³åã‚„æ§‹é€ ã«å¿œã˜ãŸå¯¾å¿œï¼‰

---

å¿…è¦ãŒã‚ã‚Œã°ã€ã“ã®å†…å®¹ã‚’ **Markdownãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆREADMEã‚„é–‹ç™ºç”¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰ã¨ã—ã¦å‡ºåŠ›**ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚ã”å¸Œæœ›ãŒã‚ã‚Œã°ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚
